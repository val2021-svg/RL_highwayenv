{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wUcySoBkQd7"
      },
      "outputs": [],
      "source": [
        "# Install environment and agent\n",
        "!pip install highway-env\n",
        "# TODO: we use the bleeding edge version because the current stable version does not support the latest gym>=0.21 versions. Revert back to stable at the next SB3 release.\n",
        "!pip install git+https://github.com/DLR-RM/stable-baselines3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbA57GUZk0Mb"
      },
      "outputs": [],
      "source": [
        "# Environment\n",
        "import gymnasium as gym\n",
        "import highway_env\n",
        "# Agent\n",
        "from stable_baselines3 import DQN\n",
        "\n",
        "# Visualization utils\n",
        "%load_ext tensorboard\n",
        "import sys\n",
        "from tqdm.notebook import trange\n",
        "!pip install tensorboardx gym pyvirtualdisplay\n",
        "!apt-get install -y xvfb ffmpeg\n",
        "!git clone https://github.com/Farama-Foundation/HighwayEnv.git 2> /dev/null\n",
        "sys.path.insert(0, '/content/HighwayEnv/scripts/')\n",
        "from utils import record_videos, show_videos\n",
        "from tqdm import trange\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_oj3Rf0pB-t"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\")\n",
        "\n",
        "config = {\n",
        "    \"observation\": {\n",
        "        \"type\": \"OccupancyGrid\",\n",
        "        \"vehicles_count\": 10,\n",
        "        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
        "        \"features_range\": {\n",
        "            \"x\": [-100, 100],\n",
        "            \"y\": [-100, 100],\n",
        "            \"vx\": [-20, 20],\n",
        "            \"vy\": [-20, 20],\n",
        "        },\n",
        "        \"grid_size\": [[-20, 20], [-20, 20]],\n",
        "        \"grid_step\": [5, 5],\n",
        "        \"absolute\": False,\n",
        "    },\n",
        "    \"action\": {\n",
        "        \"type\": \"DiscreteAction\",\n",
        "    },\n",
        "    \"lanes_count\": 3,\n",
        "    \"vehicles_count\": 10,\n",
        "    \"duration\": 20,  # [s]\n",
        "    \"initial_spacing\": 0,\n",
        "    \"collision_reward\": -1,  # The reward received when colliding with a vehicle.\n",
        "    \"right_lane_reward\": 0.5,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
        "    # zero for other lanes.\n",
        "    \"high_speed_reward\": 0.1,  # The reward received when driving at full speed, linearly mapped to zero for\n",
        "    # lower speeds according to config[\"reward_speed_range\"].\n",
        "    \"lane_change_reward\": 0,\n",
        "    \"reward_speed_range\": [\n",
        "        20,\n",
        "        30,\n",
        "    ],  # [m/s] The reward for high speed is mapped linearly from this range to [0, HighwayEnv.HIGH_SPEED_REWARD].\n",
        "    \"simulation_frequency\": 5,  # [Hz]\n",
        "    \"policy_frequency\": 1,  # [Hz]\n",
        "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\",\n",
        "    \"screen_width\": 600,  # [px]\n",
        "    \"screen_height\": 150,  # [px]\n",
        "    \"centering_position\": [0.3, 0.5],\n",
        "    \"scaling\": 5.5,\n",
        "    \"show_trajectories\": True,\n",
        "    \"render_agent\": True,\n",
        "    \"offscreen_rendering\": False,\n",
        "    \"disable_collision_checks\": True,\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "env.unwrapped.configure(config)\n",
        "print(env.reset()[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2bu0Ma0pWjx"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"A buffer for storing trajectory experiences for the DQN agent.\"\"\"\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        \"\"\"Initialize the ReplayBuffer.\n",
        "\n",
        "        Args:\n",
        "            capacity (int): The size of the replay buffer.\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, terminated, next_state):\n",
        "        \"\"\"Saves a transition to the replay buffer.\n",
        "\n",
        "        Args:\n",
        "            state: Current state of the environment.\n",
        "            action: Action taken in the state.\n",
        "            reward: Reward received after taking action.\n",
        "            terminated: Boolean indicating if the episode has ended.\n",
        "            next_state: The next state of the environment.\n",
        "        \"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = (state, action, reward, terminated, next_state)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Samples a batch of transitions from the buffer.\n",
        "\n",
        "        Args:\n",
        "            batch_size (int): Size of the sample batch.\n",
        "\n",
        "        Returns:\n",
        "            list: A batch of transitions.\n",
        "        \"\"\"\n",
        "        return random.choices(self.memory, k=batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)\n",
        "\n",
        "class CNNDQN(nn.Module):\n",
        "    \"\"\"Convolutional neural network module used in the DQN agent.\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        \"\"\"Initialize the CNNDQN model.\n",
        "\n",
        "        Args:\n",
        "            input_shape (tuple): Shape of the input observations.\n",
        "            n_actions (int): Number of possible actions.\n",
        "        \"\"\"\n",
        "        super(CNNDQN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        self.fc_input_dim = self.feature_size(input_shape)\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(self.fc_input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Output tensor.\n",
        "        \"\"\"\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "    def feature_size(self, input_shape):\n",
        "        \"\"\"Calculate the size of the feature maps after convolution layers.\n",
        "\n",
        "        Args:\n",
        "            input_shape (tuple): The shape of the input.\n",
        "\n",
        "        Returns:\n",
        "            int: The size of the feature maps.\n",
        "        \"\"\"\n",
        "        return self.conv_layers(torch.zeros(1, *input_shape)).view(1, -1).size(1)\n",
        "\n",
        "\n",
        "class DQN:\n",
        "    \"\"\"A DQN agent with convolutional neural network for function approximation.\"\"\"\n",
        "\n",
        "    def __init__(self, action_space, observation_space, gamma, batch_size, buffer_capacity, update_target_every, epsilon_start, decrease_epsilon_factor, epsilon_min, learning_rate, decay_rate, decay_step_size):\n",
        "        \"\"\"Initialize the DQN model.\n",
        "\n",
        "        Args:\n",
        "            action_space: The space of the available actions.\n",
        "            observation_space: The space of the possible states.\n",
        "            gamma (float): Discount factor for future rewards.\n",
        "            batch_size (int): Size of the batch.\n",
        "            buffer_capacity (int): Capacity of the replay buffer.\n",
        "            update_target_every (int): Frequency of updating the target network.\n",
        "            epsilon_start (float): Starting value of epsilon for epsilon-greedy strategy.\n",
        "            decrease_epsilon_factor (int): The factor used to decrease epsilon.\n",
        "            epsilon_min (float): The minimum value of epsilon.\n",
        "            learning_rate (float): Learning rate for optimizer.\n",
        "            decay_rate (float): Rate of decay for learning rate.\n",
        "            decay_step_size (int): Step size for learning rate decay.\n",
        "        \"\"\"\n",
        "        self.action_space = action_space\n",
        "        self.observation_space = observation_space\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        self.update_target_every = update_target_every\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.decrease_epsilon_factor = decrease_epsilon_factor\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.learning_rate = learning_rate\n",
        "        self.decay_rate = decay_rate\n",
        "        self.decay_step_size = decay_step_size\n",
        "        self.reset()\n",
        "\n",
        "    def update(self, state, action, reward, terminated, next_state):\n",
        "        # Ensure states are properly shaped when passed to the network\n",
        "        state = np.reshape(state, (7, 8, 8))  # Assuming the state needs to be reshaped to (C, H, W)\n",
        "        next_state = np.reshape(next_state, (7, 8, 8))\n",
        "\n",
        "        # Convert numpy arrays to tensors\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
        "        action = torch.tensor([[action]], dtype=torch.int64)\n",
        "        reward = torch.tensor([reward], dtype=torch.float32)\n",
        "        terminated = torch.tensor([terminated], dtype=torch.float32)\n",
        "\n",
        "        # Push the reshaped data into the buffer\n",
        "        self.buffer.push(state, action, reward, terminated, next_state)\n",
        "\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return np.inf  # Not enough samples to perform a batch update\n",
        "\n",
        "        # Sample a batch from the replay buffer\n",
        "        transitions = self.buffer.sample(self.batch_size)\n",
        "        batch = tuple(torch.cat(data) for data in zip(*transitions))\n",
        "\n",
        "        # Unpack the batch data\n",
        "        state_batch, action_batch, reward_batch, terminated_batch, next_state_batch = batch\n",
        "\n",
        "        # Ensure the batch data is in correct shape; e.g., (batch_size, C, H, W)\n",
        "        state_batch = state_batch.view(-1, 7, 8, 8)  # Adjust the view as per the actual state dimensions\n",
        "        next_state_batch = next_state_batch.view(-1, 7, 8, 8)\n",
        "\n",
        "        # Forward pass through the Q-network\n",
        "        q_values = self.q_net(state_batch).gather(1, action_batch)\n",
        "        next_q_values = self.target_net(next_state_batch).max(1)[0].detach()\n",
        "        targets = reward_batch + self.gamma * next_q_values * (1 - terminated_batch)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = self.loss_function(q_values, targets.unsqueeze(1))\n",
        "\n",
        "        # Backpropagation\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), max_norm=1.0)\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "\n",
        "        # Periodically update the target network\n",
        "        if self.n_steps % self.update_target_every == 0:\n",
        "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "        self.decrease_epsilon()\n",
        "        self.n_steps += 1\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "\n",
        "    def get_action(self, state, use_thompson_sampling=False):\n",
        "        \"\"\"\n",
        "        Return action based on Thompson Sampling or epsilon-greedy policy.\n",
        "        \"\"\"\n",
        "        channels = 7  # Number of features\n",
        "        height = 8\n",
        "        width = 8\n",
        "        state = np.reshape(state, (channels, height, width))\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        if use_thompson_sampling:\n",
        "            q_values = self.q_net(state)\n",
        "            action = torch.argmax(q_values).item()\n",
        "        else:\n",
        "            if np.random.rand() < self.epsilon:\n",
        "                action = self.action_space.sample()\n",
        "            else:\n",
        "                q_values = self.q_net(state)\n",
        "                action = torch.argmax(q_values).item()\n",
        "        return action\n",
        "\n",
        "    def get_q(self, state):\n",
        "      \"\"\"\n",
        "      Pass the state through the network to get Q-values for all possible actions.\n",
        "      \"\"\"\n",
        "      with torch.no_grad():\n",
        "          return self.q_net(state)\n",
        "\n",
        "    def decrease_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * np.exp(-1.0 / self.decrease_epsilon_factor))\n",
        "        self.n_eps += 1\n",
        "\n",
        "    def reset(self):\n",
        "        n_actions = self.action_space.n\n",
        "        obs_size = self.observation_space.shape  # (channels, height, width)\n",
        "        self.buffer = ReplayBuffer(self.buffer_capacity)\n",
        "        print(\"obs_size\", obs_size)\n",
        "        print(\"n_actions\", n_actions.shape)\n",
        "        self.q_net = CNNDQN(obs_size, n_actions)\n",
        "        self.target_net = CNNDQN(obs_size, n_actions)\n",
        "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "        self.loss_function = nn.SmoothL1Loss()\n",
        "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=self.learning_rate)\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=self.decay_step_size, gamma=self.decay_rate)\n",
        "        self.epsilon = self.epsilon_start\n",
        "        self.n_steps = 0\n",
        "        self.n_eps = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJ1pIJw_ppob"
      },
      "outputs": [],
      "source": [
        "def eval_agent(agent, env, n_sim, use_thompson_sampling=False):\n",
        "    \"\"\"\n",
        "    Monte Carlo evaluation of DQN agent.\n",
        "\n",
        "    Repeat n_sim times:\n",
        "        * Run the DQN policy until the environment reaches a terminal state (= one episode)\n",
        "        * Compute the sum of rewards in this episode\n",
        "        * Store the sum of rewards in the episode_rewards array.\n",
        "    \"\"\"\n",
        "    episode_rewards = np.zeros(n_sim)\n",
        "    for i in range(n_sim):\n",
        "        state, _ = env.reset()  # Assuming env.reset() now returns a tuple (state, info)\n",
        "        state = state.flatten()  # Flattening the state to match the expected input dimension for the agent\n",
        "        reward_sum = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.get_action(state, use_thompson_sampling=use_thompson_sampling)\n",
        "            #action = agent.get_action(state, 0)  # Using epsilon=0 to exploit the best known strategy\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            state = state.flatten()  # Update the state dimensionality after each step\n",
        "            reward_sum += reward\n",
        "            done = terminated or truncated\n",
        "        episode_rewards[i] = reward_sum\n",
        "    return episode_rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_wo2ohRqE8H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming the agent and environment are correctly initialized and configured\n",
        "action_space = env.action_space\n",
        "observation_space = env.observation_space\n",
        "\n",
        "# Initialize hyperparameters for the DQN agent.\n",
        "gamma = 0.9  # Discount factor for future rewards (0.8 and 0.95 resulted in worse performance).\n",
        "batch_size = 128  # Size of the batch used in the training of the DQN.\n",
        "buffer_capacity = 20_000  # Size of the replay buffer.\n",
        "update_target_every = 100  # Frequency of target network update (better results than with 50 or 32).\n",
        "\n",
        "epsilon_start = 0.9  # Initial probability for epsilon-greedy policy.\n",
        "decrease_epsilon_factor = 500  # Factor to decrease epsilon.\n",
        "epsilon_min = 0.05  # Minimum value for epsilon.\n",
        "\n",
        "# Learning rate decay configuration.\n",
        "decay_rate = 0.95  # Decay the learning rate by this rate.\n",
        "decay_step_size = 1000  # Number of steps before the learning rate decays.\n",
        "\n",
        "n_sim = 10  # Number of simulations for evaluation.\n",
        "learning_rate = 5e-4  # Learning rate for the optimizer.\n",
        "\n",
        "# Aggregate all hyperparameters into a single tuple for passing to the DQN agent.\n",
        "arguments = (\n",
        "    action_space,\n",
        "    observation_space,\n",
        "    gamma,\n",
        "    batch_size,\n",
        "    buffer_capacity,\n",
        "    update_target_every,\n",
        "    epsilon_start,\n",
        "    decrease_epsilon_factor,\n",
        "    epsilon_min,\n",
        "    learning_rate,\n",
        "    decay_rate,\n",
        "    decay_step_size,\n",
        ")\n",
        "\n",
        "N_episodes = 1000  # Number of training episodes.\n",
        "\n",
        "# Create an instance of the DQN agent with the specified arguments.\n",
        "agent = DQN(*arguments)\n",
        "\n",
        "def train(env, agent, N_episodes, eval_every=10, reward_threshold=20, print_every=10):\n",
        "    \"\"\"Train the DQN agent.\n",
        "\n",
        "    Args:\n",
        "        env: The environment in which the agent operates.\n",
        "        agent: The DQN agent to train.\n",
        "        N_episodes (int): Number of episodes to train the agent for.\n",
        "        eval_every (int): Frequency of evaluations during training.\n",
        "        reward_threshold (float): Threshold for reward to determine when to stop training.\n",
        "        print_every (int): How often to print training progress.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple containing the losses and mean rewards for each episode.\n",
        "    \"\"\"\n",
        "    total_time = 0  # Initialize the total time of training.\n",
        "    state, _ = env.reset()  # Reset the environment state.\n",
        "    losses = []  # Initialize the list to store losses per episode.\n",
        "    episode_rewards = []  # Initialize the list to store rewards per episode.\n",
        "    mean_rewards = []  # Initialize the list to store mean rewards for plotting.\n",
        "    episode_lengths = []  # Initialize the list to store the length of each episode.\n",
        "\n",
        "    for ep in range(N_episodes):\n",
        "        done = False\n",
        "        state, _ = env.reset()\n",
        "        state = state.flatten()  # Flatten the state for processing.\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "\n",
        "        while not done:\n",
        "            use_thompson = np.random.rand() < 0.1  # Decide whether to use Thompson Sampling.\n",
        "            action = agent.get_action(state, use_thompson_sampling=use_thompson)\n",
        "\n",
        "            # Take the action in the environment and observe the next state and reward.\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            next_state = next_state.flatten().astype(np.float32)  # Flatten the next state.\n",
        "            loss_val = agent.update(state, action, reward, terminated, next_state)\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            losses.append(loss_val)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            episode_length += 1\n",
        "\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_lengths.append(episode_length)\n",
        "\n",
        "        if ((ep + 1) % eval_every == 0):\n",
        "            # Evaluate the agent's performance after 'eval_every' episodes.\n",
        "            rewards = eval_agent(agent, env, n_sim)\n",
        "            mean_reward = np.mean(rewards)\n",
        "            mean_rewards.append(mean_reward)\n",
        "            print(f\"Episode = {ep + 1}, Reward = {mean_reward}, Loss = {loss_val}, Epsilon = {agent.epsilon}\")\n",
        "\n",
        "            if mean_reward >= reward_threshold:\n",
        "                # Stop training if the mean reward exceeds the reward threshold.\n",
        "                print(\"Reward threshold met, stopping training.\")\n",
        "                break\n",
        "\n",
        "    return losses, mean_rewards\n",
        "\n",
        "# Run the training loop\n",
        "losses, mean_rewards = train(env, agent, N_episodes, print_every=10)\n",
        "\n",
        "# Plot the training losses over episodes.\n",
        "plt.plot(losses)\n",
        "plt.title(\"Training Losses Over Episodes\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the final policy of the agent after training.\n",
        "rewards = eval_agent(agent, env, n_sim)\n",
        "print(\"\\nmean reward after training = \", np.mean(rewards))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_mean_rewards(mean_rewards, eval_every=10):\n",
        "    \"\"\"\n",
        "    Plot the mean rewards over training episodes at specified evaluation intervals.\n",
        "\n",
        "    Args:\n",
        "        mean_rewards (list): List of mean rewards corresponding to evaluation points.\n",
        "        eval_every (int): Interval of episodes between evaluations.\n",
        "    \"\"\"\n",
        "    # Set up the figure size for the plot\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Generate evaluation points based on the frequency of evaluations\n",
        "    eval_points = np.arange(eval_every, eval_every * len(mean_rewards) + 1, eval_every)\n",
        "\n",
        "    # Plot mean rewards per evaluation interval\n",
        "    plt.plot(eval_points, mean_rewards, label='Mean Reward per Eval')\n",
        "\n",
        "    # Check if mean_rewards is not empty\n",
        "    if mean_rewards:\n",
        "        # Find the first peak in mean rewards\n",
        "        for i in range(1, len(mean_rewards) - 1):\n",
        "            # A peak is defined as a point where the reward is higher than the rewards at adjacent points\n",
        "            if mean_rewards[i] > mean_rewards[i - 1] and mean_rewards[i] > mean_rewards[i + 1]:\n",
        "                first_peak_index = i\n",
        "                break\n",
        "        else:\n",
        "            # If no peak is found, default to the first index\n",
        "            first_peak_index = 0\n",
        "\n",
        "        # Calculate the average reward from the point after the first peak to the end of the list\n",
        "        mean_reward_after_peak = np.mean(mean_rewards[first_peak_index + 1:])\n",
        "        # Draw a horizontal line representing this average reward value\n",
        "        plt.axhline(y=mean_reward_after_peak, color='r', linestyle='--', label=f'Mean Reward After First Peak: {mean_reward_after_peak:.2f}')\n",
        "\n",
        "    # Add title and labels with font sizes adjusted\n",
        "    plt.title('Mean Rewards Over Training Episodes', fontsize=18)\n",
        "    plt.xlabel('Episode Number', fontsize=12)\n",
        "    plt.ylabel('Mean Reward', fontsize=12)\n",
        "\n",
        "    # Set x-ticks to display every 100 episodes, based on evaluation points\n",
        "    tick_interval = 100  # Define the interval for x-ticks\n",
        "    x_ticks = eval_points[::tick_interval // eval_every]\n",
        "    plt.xticks(x_ticks)\n",
        "\n",
        "    # Display the legend and grid, and adjust layout\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()  # Adjust layout to prevent clipping of tick labels\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5-xC6o37xbpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9Se7sIFric3"
      },
      "outputs": [],
      "source": [
        "# Initialize the environment with video recording capabilities\n",
        "# Uncomment the appropriate line depending on the environment setup\n",
        "# env = gym.make('highway-fast-v0', render_mode='rgb_array')\n",
        "env = record_videos(env)  # Wraps the environment to record videos\n",
        "\n",
        "# Run a number of episodes for testing the agent\n",
        "for episode in trange(1, desc='Test episodes'):\n",
        "    # Reset the environment and get the initial observation and info, marking 'done' as False\n",
        "    (obs, info), done = env.reset(), False\n",
        "    # Flatten the observation for compatibility with the agent's expected input format\n",
        "    obs = obs.flatten()\n",
        "\n",
        "    # Initialize a counter to limit the number of steps per episode\n",
        "    i = 0\n",
        "    # Loop until the episode is done or the step limit is reached\n",
        "    while not done and i < 1000:\n",
        "        # Select an action using the agent's policy\n",
        "        action = agent.get_action(obs, agent.epsilon)\n",
        "        # Execute the action in the environment and observe the results\n",
        "        obs, reward, done, truncated, info = env.step(int(action))\n",
        "        # Flatten the observation again\n",
        "        obs = obs.flatten()\n",
        "        # Increment the step counter\n",
        "        i += 1\n",
        "\n",
        "# Close the environment to clean up resources\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyO15PoMua3z"
      },
      "outputs": [],
      "source": [
        "show_videos()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}